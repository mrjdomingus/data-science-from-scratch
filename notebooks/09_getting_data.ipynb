{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just stick some data there\n",
    "with open('email_addresses.txt', 'w') as f:\n",
    "    f.write(\"joelgrus@gmail.com\\n\")\n",
    "    f.write(\"joel@m.datasciencester.com\\n\")\n",
    "    f.write(\"joelgrus@m.datasciencester.com\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(email_address: str) -> str:\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "# a couple of tests\n",
    "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
    "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('email_addresses.txt', 'r') as f:\n",
    "    domain_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if \"@\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tab_delimited_stock_prices.txt', 'w') as f:\n",
    "    f.write(\"\"\"6/20/2014\\tAAPL\\t90.91\n",
    "6/20/2014\\tMSFT\\t41.68\n",
    "6/20/2014\\tFB\\t64.5\n",
    "6/19/2014\\tAAPL\\t91.86\n",
    "6/19/2014\\tMSFT\\t41.51\n",
    "6/19/2014\\tFB\\t64.34\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(date: str, symbol: str, closing_price: float) -> None:\n",
    "    # Imaginge that this function actually does something.\n",
    "    assert closing_price > 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tab_delimited_stock_prices.txt') as f:\n",
    "    tab_reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in tab_reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('colon_delimited_stock_prices.txt', 'w') as f:\n",
    "    f.write(\"\"\"date:symbol:closing_price\n",
    "6/20/2014:AAPL:90.91\n",
    "6/20/2014:MSFT:41.68\n",
    "6/20/2014:FB:64.5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('colon_delimited_stock_prices.txt') as f:\n",
    "    colon_reader = csv.DictReader(f, delimiter=':')\n",
    "    for dict_row in colon_reader:\n",
    "        date = dict_row[\"date\"]\n",
    "        symbol = dict_row[\"symbol\"]\n",
    "        closing_price = float(dict_row[\"closing_price\"])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in todays_prices.items():\n",
    "        csv_writer.writerow([stock, price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[\"test1\", \"success\", \"Monday\"],\n",
    "           [\"test2\", \"success, kind of\", \"Tuesday\"],\n",
    "           [\"test3\", \"failure, kind of\", \"Wednesday\"],\n",
    "           [\"test4\", \"failure, utter\", \"Thursday\"]]\n",
    "\n",
    "# don't do this!\n",
    "with open('bad_csv.txt', 'w') as f:\n",
    "    for row in results:\n",
    "        f.write(\",\".join(map(str, row))) # might have too many commas in it!\n",
    "        f.write(\"\\n\")                    # row might have newlines as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I put the relevant HTML file on GitHub. In order to fit\n",
    "# the URL in the book I had to split it across two lines.\n",
    "# Recall that whitespace-separated strings get concatenated.\n",
    "url = (\"https://raw.githubusercontent.com/\"\n",
    "       \"joelgrus/data/master/getting-data.html\")\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "first_paragraph = soup.find('p')        # or just soup.p\n",
    "\n",
    "\n",
    "assert str(soup.find('p')) == '<p id=\"p1\">This is the first paragraph.</p>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "\n",
    "assert first_paragraph_words == ['This', 'is', 'the', 'first', 'paragraph.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\n",
    "first_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n",
    "\n",
    "\n",
    "assert first_paragraph_id == first_paragraph_id2 == 'p1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = soup.find_all('p')  # or just soup('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "\n",
    "assert len(all_paragraphs) == 2\n",
    "assert len(paragraphs_with_ids) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_paragraphs = soup('p', {'class' : 'important'})\n",
    "important_paragraphs2 = soup('p', 'important')\n",
    "important_paragraphs3 = [p for p in soup('p')\n",
    "                         if 'important' in p.get('class', [])]\n",
    "\n",
    "\n",
    "assert important_paragraphs == important_paragraphs2 == important_paragraphs3\n",
    "assert len(important_paragraphs) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning, will return the same span multiple times\n",
    "# if it sits inside multiple divs\n",
    "# be more clever if that's the case\n",
    "spans_inside_divs = [span\n",
    "                     for div in soup('div')     # for each <div> on the page\n",
    "                     for span in div('span')]   # find each <span> inside it\n",
    "\n",
    "\n",
    "assert len(spans_inside_divs) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if a <p> inside the text mentions {keyword}\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    paragraphs = [p.get_text() for p in soup('p')]\n",
    "\n",
    "    return any(keyword.lower() in paragraph.lower()\n",
    "               for paragraph in paragraphs)\n",
    "\n",
    "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
    "assert paragraph_mentions(text, \"twitter\")       # is inside a <p>\n",
    "assert not paragraph_mentions(text, \"facebook\")  # not inside a <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2019,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "# parse the JSON to create a Python dict\n",
    "deserialized = json.loads(serialized)\n",
    "assert deserialized[\"publicationYear\"] == 2019\n",
    "assert \"data science\" in deserialized[\"topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    \n",
    "    url = \"https://www.house.gov/representatives\"\n",
    "    text = requests.get(url).text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    all_urls = [a['href']\n",
    "                for a in soup('a')\n",
    "                if a.has_attr('href')]\n",
    "    \n",
    "    print(len(all_urls))  # 965 for me, way too many\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Must start with http:// or https://\n",
    "    # Must end with .house.gov or .house.gov/\n",
    "    regex = r\"^https?://.*\\.house\\.gov/?$\"\n",
    "    \n",
    "    # Let's write some tests!\n",
    "    assert re.match(regex, \"http://joel.house.gov\")\n",
    "    assert re.match(regex, \"https://joel.house.gov\")\n",
    "    assert re.match(regex, \"http://joel.house.gov/\")\n",
    "    assert re.match(regex, \"https://joel.house.gov/\")\n",
    "    assert not re.match(regex, \"joel.house.gov\")\n",
    "    assert not re.match(regex, \"http://joel.house.com\")\n",
    "    assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
    "    \n",
    "    # And now apply\n",
    "    good_urls = [url for url in all_urls if re.match(regex, url)]\n",
    "    \n",
    "    print(len(good_urls))  # still 862 for me\n",
    "    \n",
    "    \n",
    "    num_original_good_urls = len(good_urls)\n",
    "    \n",
    "    good_urls = list(set(good_urls))\n",
    "    \n",
    "    print(len(good_urls))  # only 431 for me\n",
    "    \n",
    "    \n",
    "    assert len(good_urls) < num_original_good_urls\n",
    "    \n",
    "    html = requests.get('https://jayapal.house.gov').text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Use a set because the links might appear multiple times.\n",
    "    links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "    \n",
    "    print(links) # {'/media/press-releases'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # I don't want this file to scrape all 400+ websites every time it runs.\n",
    "    # So I'm going to randomly throw out most of the urls.\n",
    "    # The code in the book doesn't do this.\n",
    "    import random\n",
    "    good_urls = random.sample(good_urls, 5)\n",
    "    print(f\"after sampling, left with {good_urls}\")\n",
    "    \n",
    "    from typing import Dict, Set\n",
    "    \n",
    "    press_releases: Dict[str, Set[str]] = {}\n",
    "    \n",
    "    for house_url in good_urls:\n",
    "        html = requests.get(house_url).text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        pr_links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
    "        print(f\"{house_url}: {pr_links}\")\n",
    "        press_releases[house_url] = pr_links\n",
    "    \n",
    "    for house_url, pr_links in press_releases.items():\n",
    "        for pr_link in pr_links:\n",
    "            url = f\"{house_url}/{pr_link}\"\n",
    "            text = requests.get(url).text\n",
    "    \n",
    "            if paragraph_mentions(text, 'data'):\n",
    "                print(f\"{house_url}\")\n",
    "                break  # done with this house_url\n",
    "    \n",
    "    import requests, json\n",
    "    \n",
    "    github_user = \"joelgrus\"\n",
    "    endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
    "    \n",
    "    repos = json.loads(requests.get(endpoint).text)\n",
    "    \n",
    "    from collections import Counter\n",
    "    from dateutil.parser import parse\n",
    "    \n",
    "    dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "    month_counts = Counter(date.month for date in dates)\n",
    "    weekday_counts = Counter(date.weekday() for date in dates)\n",
    "    \n",
    "    last_5_repositories = sorted(repos,\n",
    "                                 key=lambda r: r[\"pushed_at\"],\n",
    "                                 reverse=True)[:5]\n",
    "    \n",
    "    last_5_languages = [repo[\"language\"]\n",
    "                        for repo in last_5_repositories]\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Feel free to plug your key and secret in directly\n",
    "    CONSUMER_KEY = os.environ.get(\"TWITTER_CONSUMER_KEY\")\n",
    "    CONSUMER_SECRET = os.environ.get(\"TWITTER_CONSUMER_SECRET\")\n",
    "    \n",
    "    import webbrowser\n",
    "    from twython import Twython\n",
    "    \n",
    "    # Get a temporary client to retrieve an authentication url\n",
    "    temp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    temp_creds = temp_client.get_authentication_tokens()\n",
    "    url = temp_creds['auth_url']\n",
    "    \n",
    "    # Now visit that URL to authorize the application and get a PIN\n",
    "    print(f\"go visit {url} and get the PIN code and paste it below\")\n",
    "    webbrowser.open(url)\n",
    "    PIN_CODE = input(\"please enter the PIN code: \")\n",
    "    \n",
    "    # Now we use that PIN_CODE to get the actual tokens\n",
    "    auth_client = Twython(CONSUMER_KEY,\n",
    "                          CONSUMER_SECRET,\n",
    "                          temp_creds['oauth_token'],\n",
    "                          temp_creds['oauth_token_secret'])\n",
    "    final_step = auth_client.get_authorized_tokens(PIN_CODE)\n",
    "    ACCESS_TOKEN = final_step['oauth_token']\n",
    "    ACCESS_TOKEN_SECRET = final_step['oauth_token_secret']\n",
    "    \n",
    "    # And get a new Twython instance using them.\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    \n",
    "    from twython import TwythonStreamer\n",
    "    \n",
    "    # Appending data to a global variable is pretty poor form\n",
    "    # but it makes the example much simpler\n",
    "    tweets = []\n",
    "    \n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            \"\"\"\n",
    "            What do we do when twitter sends us data?\n",
    "            Here data will be a Python dict representing a tweet\n",
    "            \"\"\"\n",
    "            # We only want to collect English-language tweets\n",
    "            if data.get('lang') == 'en':\n",
    "                tweets.append(data)\n",
    "                print(f\"received tweet #{len(tweets)}\")\n",
    "    \n",
    "            # Stop when we've collected enough\n",
    "            if len(tweets) >= 100:\n",
    "                self.disconnect()\n",
    "    \n",
    "        def on_error(self, status_code, data):\n",
    "            print(status_code, data)\n",
    "            self.disconnect()\n",
    "    \n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    \n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')\n",
    "    \n",
    "    # if instead we wanted to start consuming a sample of *all* public statuses\n",
    "    # stream.statuses.sample()\n",
    "    \n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
